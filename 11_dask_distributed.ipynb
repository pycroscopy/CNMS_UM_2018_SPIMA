{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USID and Dask.distributed\n",
    "***\n",
    "Authors: **Emily Costa, Suhas Somnath**\n",
    "\n",
    "Created on: 7/30/19\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Certain data processing routines are very time consuming because of the sheer size of the data and/or the computational complexity of the data processing routine. Often, such computations are ``embarrasingly parallel`` meaning that the processing of one portion (e.g. pixel) of data is independent from  the processing of all other portions of data.\n",
    "\n",
    "The `pyUSID.parallel_compute()` function can effectively distribute the computation over all available cores in a CPU and reduce the computational time. However, `pyUSID.parallel_compute()` only distributes computations within a single CPU in a single personal computer. As a consequence, it may not be feasible to run large / lengthy computations on personal computers. In such cases and when available, it is recommended that such computations be run on a university / national lab compute cluster for timely processing of the data.\n",
    "\n",
    "This document will demonstrate how `dask.distributed` can be used with USID files managed with pyUSID. `dask.distributed` is a python library for distributed computing. It is a task scheduler that is compatible with other Dask libraries and is meant to install and run intuitively. It uses familiar APIs and is built in Python.\n",
    "\n",
    "During this tutorial, we will be using `find_all_peaks` as an example for computing on USID main datasets using different libraries and techniques. `find_all_peaks` is a simple function that takes a one-dimensional array and finds all local maxima by simple comparison of neighbouring values.\n",
    "\n",
    "We will compare the performace of serial computation, dask.distributed, and parallel_compute by plotting the performance benchmarks and cores used while computing the \"peaks\" of all the arrays in a USID main dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import, unicode_literals\n",
    "\n",
    "# The package for accessing files in directories, etc.:\n",
    "import os\n",
    "\n",
    "# Warning package in case something goes wrong\n",
    "from warnings import warn\n",
    "import subprocess\n",
    "#from dask.diagnostics import Profiler, ResourceProfiler, visualize\n",
    "import dask.array as da\n",
    "\n",
    "\n",
    "def install(package):\n",
    "    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "# Package for downloading online files:\n",
    "try:\n",
    "    # This package is not part of anaconda and may need to be installed.\n",
    "    import wget\n",
    "except ImportError:\n",
    "    warn('wget not found.  Will install with pip.')\n",
    "    import pip\n",
    "    install(wget)\n",
    "    import wget\n",
    "\n",
    "# The mathematical computation package:\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.array as da\n",
    "\n",
    "# The package used for creating and manipulating HDF5 files:\n",
    "import h5py\n",
    "\n",
    "# Packages for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parallel computation library:\n",
    "try:\n",
    "    import joblib\n",
    "except ImportError:\n",
    "    warn('joblib not found.  Will install with pip.')\n",
    "    import pip\n",
    "    install('joblib')\n",
    "    import joblib\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# A handy python utility that allows us to preconfigure parts of a function\n",
    "from functools import partial\n",
    "\n",
    "# Finally import pyUSID:\n",
    "try:\n",
    "    import pyUSID as usid\n",
    "except ImportError:\n",
    "    warn('pyUSID not found.  Will install with pip.')\n",
    "    import pip\n",
    "    install('pyUSID')\n",
    "    import pyUSID as usid\n",
    "    \n",
    "\n",
    "# import the scientific function:\n",
    "import sys\n",
    "sys.path.append('./supporting_docs/')\n",
    "from peak_finding import find_all_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find main dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/pycroscopy/pyUSID/master/data/BELine_0004.h5'\n",
    "h5_path = 'temp.h5'\n",
    "if os.path.exists(h5_path):\n",
    "    os.remove(h5_path)\n",
    "_ = wget.download(url, h5_path, bar=None)\n",
    "    # Open the file in read-only mode\n",
    "h5_file = h5py.File(h5_path, mode='r')\n",
    "    # Get handle to the the raw data\n",
    "h5_meas_grp = h5_file['Measurement_000']\n",
    "    # Accessing the dataset of interest:\n",
    "h5_main = usid.USIDataset(h5_meas_grp['Channel_000/Raw_Data'])\n",
    "num_rows, num_cols = h5_main.pos_dim_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test find_all_peaks\n",
    "\n",
    "In order to test the function, we will run it on one of the arrays in the main dataset of our sample data. \n",
    "\n",
    "Test the `find_all_peaks` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose an arbitrary pixel_ind to extract an array from the data\n",
    "row_ind, col_ind = 110, 25\n",
    "pixel_ind = col_ind + row_ind * num_cols\n",
    "spectra = h5_main[pixel_ind]\n",
    "\n",
    "# run function on the array and return peaks\n",
    "peak_inds = find_all_peaks(spectra, [20, 60], num_steps=30)\n",
    "\n",
    "# visualize \n",
    "fig, axis = plt.subplots()\n",
    "axis.scatter(np.arange(len(spectra)), np.abs(spectra), c='black')\n",
    "axis.axvline(peak_inds[0], color='r', linewidth=2)\n",
    "axis.set_ylim([0, 1.1 * np.max(np.abs(spectra))]);\n",
    "axis.set_title('find_all_peaks found peaks at index: {}'.format(peak_inds), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Serially\n",
    "The following cell demonstrates the ineffectiveness of serial computation in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "raw_data = h5_main[()]\n",
    "serial_results = list()\n",
    "for vector in raw_data:\n",
    "    serial_results.append(find_all_peaks(vector, [20, 60], num_steps=30))\n",
    "time_serial = time.time()-start\n",
    "core_serial = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute using pyUSID parallel_compute() function\n",
    "Now that we have completed the tedious serial computation, we will run parallel computation using pyUSID's `parallel_compute()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time for benchmarking\n",
    "start = time.time()\n",
    "# create an array with all the data\n",
    "raw_data = h5_main[()]\n",
    "# width_bounds and num_steps parameters\n",
    "args = [[20, 60]]\n",
    "kwargs = {'num_steps': 30}\n",
    "# specify cores\n",
    "cores = 3\n",
    "# run parallel computation\n",
    "parallel_results = usid.parallel_compute(raw_data, find_all_peaks, cores=cores, \n",
    "                                         func_args=args, func_kwargs=kwargs)\n",
    "# for benchmarking\n",
    "time_parallel = time.time() - start\n",
    "cores_parallel = cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute using Dask:\n",
    "This will automate the scaling of cores and parallelization of the code. `dask.Client` automates the delegation of work to the workers. For this example, we need to map all the arrays in the dataset to run through the `find_all_peaks` function then collect them after computation. Before mapping the function, we should move the data from the local client process into the workers of the distributed scheduler. Moving the data will avoid time-consuming effort of the workers retrieving data from the local client.\n",
    "\n",
    "For more information on `dask.Client` and its API, visit [the documentation](https://distributed.dask.org/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "raw_data = h5_main[()]\n",
    "args = [20, 60]\n",
    "kwargs = {'num_steps': 30}\n",
    "# Instantiate the client\n",
    "client = Client(processes=False)\n",
    "# distribute the data to the workers, this should speed up the work.\n",
    "raw_data = client.scatter(raw_data)\n",
    "# map each array to the function\n",
    "L = client.map(find_all_peaks, raw_data_list, args, kwargs)\n",
    "# gather will compute on the mapping\n",
    "dask_results = client.gather(L)\n",
    "# benchmark\n",
    "cores_dask = client.ncores()\n",
    "# make sure to close the client\n",
    "client.close()\n",
    "# benchmark\n",
    "time_dask = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize=(3.5, 3.5))\n",
    "axis.scatter(core_serial, time_serial, c='blue')\n",
    "axis.scatter(cores_parallel, time_parallel, c='green')\n",
    "axis.scatter(cores_dask, time_dask, c='red')\n",
    "axis.set_xlabel('CPU cores', fontsize=14)\n",
    "axis.set_ylabel('Compute time (sec)', fontsize=14)\n",
    "plt.legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig('{}.png'.format(png_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
